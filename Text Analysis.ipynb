{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and import \"book\"\n",
    "nltk.download('book', quiet=True)\n",
    "from nltk import book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chars = set(map(lambda x: x.split('\\\\')[-1][:-4], glob('data/wow_chars/*.txt')))\n",
    "# chars_with_comments = set(map(lambda x: x.split('\\\\')[-1][:-6], glob('data/char_comments/*.njson')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in character DataFrame\n",
    "df = pd.read_csv(config.PATH_RES + 'df_chars.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK on Cleaned Wikipages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NLTK text of cleaned texts\n",
    "file_list = [fn for fn in glob(config.PATH_CLEAN + '*.txt')]\n",
    "text_corpus = nltk.corpus.PlaintextCorpusReader('', file_list)\n",
    "wiki_text = nltk.Text(text_corpus.words())\n",
    "\n",
    "# create NLTK text of words\n",
    "file_list = [fn for fn in glob(config.PATH_WORDS + '*.txt')]\n",
    "words_corpus = nltk.corpus.PlaintextCorpusReader('', file_list)\n",
    "wiki_words = nltk.Text(words_corpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worclouds of text surrounding specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find occurrances of word\n",
    "word_list = ['sword', 'shield', 'mace']\n",
    "for word in word_list:\n",
    "    print(wiki_text.concordance(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(df, attr, save=False):\n",
    "    # get unique attributes\n",
    "    attrs = df[attr].unique()\n",
    "\n",
    "    # create collection with texts for each member\n",
    "    col = {}\n",
    "    for at in attrs:\n",
    "        # create list of paths for every character of current faction\n",
    "        names = df.loc[df[attr] == at, 'Name'].values\n",
    "        paths = [\n",
    "            config.PATH_WORDS + n.replace(' ', '_') + '.txt' \n",
    "            for n in names\n",
    "        ]\n",
    "\n",
    "        # save text for faction\n",
    "        col[at] = {'text': nltk.Text(words_corpus.words(paths))}\n",
    "\n",
    "    N = len(col)\n",
    "    for split in col:\n",
    "        text = col[split]['text']\n",
    "\n",
    "        # calculate TC and TF\n",
    "        words, tc = np.unique(text, return_counts=True)\n",
    "        tf = tc / len(text)\n",
    "\n",
    "        # calculate IDF\n",
    "        IDF = []\n",
    "        for word in words:\n",
    "            n_t = 0\n",
    "            for doc in col:\n",
    "                txt = col[doc]['text']\n",
    "                if txt.count(word):\n",
    "                    n_t += 1\n",
    "            IDF.append(np.log(N / n_t))\n",
    "\n",
    "        # store stuff\n",
    "        col[split]['words'] = words\n",
    "        col[split]['tc'] = tc\n",
    "        col[split]['tf'] = tf\n",
    "        col[split]['idf'] = tf\n",
    "    \n",
    "    if save:\n",
    "        with open(config.PATH_RES + attr + '_dict.json', 'wb') as f:\n",
    "            pickle.dump(col, f)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in ['Faction', 'Gender', 'Race', 'Status']:\n",
    "    if not os.path.exists(config.PATH_RES + attr + '_dict.json'):\n",
    "        print(f'Creating collection for {attr}')\n",
    "        _ = create_collection(df, attr, save=True)\n",
    "        print(f'\\nDone with collection for {attr}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frequency distribution of the text\n",
    "fd = book.FreqDist(wiki_words)\n",
    "\n",
    "# Display the 75 most common tokens in a cumulative frequency plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.title('Cumulative Frequency plot of 75 most common words')\n",
    "fd.plot(75, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK on WoWhead User Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c8b715ef500c155f7e623c10e2a2705ee7484473bce4c48cafd25806eafb59b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
