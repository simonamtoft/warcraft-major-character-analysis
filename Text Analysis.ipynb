{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import community\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import config\n",
    "from text_helpers import init_collection, populate_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and import \"book\"\n",
    "nltk.download('book', quiet=True)\n",
    "from nltk import book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all characters which \n",
    "chars_with_comments = [\n",
    "    path.split('\\\\')[-1].replace('.njson', '') \n",
    "    for path in glob('./data/char_comments/*.njson')\n",
    "]\n",
    "\n",
    "# read in character DataFrame\n",
    "df = pd.read_csv(config.PATH_RES + 'df_chars.csv')\n",
    "\n",
    "# remove chars that doesn't have comments from wowhead\n",
    "df = df[df['Name'].apply(lambda n: n in chars_with_comments)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "Gcc = nx.read_gexf(config.PATH_RES + 'Gcc_wow.gexf').to_undirected()\n",
    "\n",
    "# remove nodes from graph that doesn't have comments from wowhead\n",
    "for node in list(Gcc.nodes()):\n",
    "    if node.replace(' ', '_') not in chars_with_comments:\n",
    "        Gcc.remove_node(node)\n",
    "\n",
    "print(f'Number of\\nNodes: {len(list(Gcc.nodes()))}\\nEdges: {len(list(Gcc.edges()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Communities\n",
    "Create or load community partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create communities if not done already, otherwise load\n",
    "filename = config.PATH_RES + 'Communities.json'\n",
    "if not os.path.isfile(filename):\n",
    "    print('Creating new community partition.')\n",
    "    partition = community.best_partition(Gcc)\n",
    "    communities = []\n",
    "    for p in set(partition.values()):\n",
    "        names = [n for n in partition if partition[n] == p]\n",
    "        communities.append(names)\n",
    "    pickle.dump(communities, open(filename, 'wb'))\n",
    "    print(f'Saved as pickle {filename}')\n",
    "else: \n",
    "    print('Loading existing community partition.')\n",
    "    print(f'from pickle {filename}')\n",
    "    communities = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# get top chars in each community\n",
    "degs = list(Gcc.degree())\n",
    "com_names = []\n",
    "for com in communities:\n",
    "    com_sorted = sorted([(n, v) for n, v in degs if n in com], key=lambda x: x[1], reverse=True)\n",
    "    top_names = [n for n, _ in com_sorted[:3]]\n",
    "    com_name = ', '.join(top_names)\n",
    "    com_names.append(com_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file list for every character\n",
    "f_wiki_words = []\n",
    "f_wiki_clean = []\n",
    "f_comments_words = []\n",
    "f_comments_clean = []\n",
    "for charname in chars_with_comments:\n",
    "    f_wiki_words.append(config.PATH_WORDS + charname + '.txt')\n",
    "    f_wiki_clean.append(config.PATH_CLEAN + charname + '.txt')\n",
    "    f_comments_words.append(config.PATH_COMMENTS_WORDS + charname + '.txt')\n",
    "    f_comments_clean.append(config.PATH_COMMENTS_CLEAN + charname + '.txt')\n",
    "\n",
    "# create corpus for wiki and wowhead\n",
    "c_words_wiki = nltk.corpus.PlaintextCorpusReader('', f_wiki_words)\n",
    "c_clean_wiki = nltk.corpus.PlaintextCorpusReader('', f_wiki_clean)\n",
    "c_words_comments = nltk.corpus.PlaintextCorpusReader('', f_comments_words)\n",
    "c_clean_comments = nltk.corpus.PlaintextCorpusReader('', f_comments_clean)\n",
    "\n",
    "# create text for wiki and wowhead\n",
    "t_words_wiki = nltk.Text(c_words_wiki.words())\n",
    "t_clean_wiki = nltk.Text(c_clean_wiki.words())\n",
    "t_words_comments = nltk.Text(c_words_comments.words())\n",
    "t_clean_comments = nltk.Text(c_clean_comments.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define what to look into\n",
    "attr_lookup = {\n",
    "    'Gender': [('Male', '#0B1C51'), ('Female', '#FCB9B2')],\n",
    "    'Faction': [('Alliance', config.COLOR_ALLIANCE), ('Horde', config.COLOR_HORDE)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create collections for attributes for both wowpedia pages and wowhead comments\n",
    "for source, corpus in [\n",
    "    ('wowpedia/', c_words_wiki), \n",
    "    ('wowhead/', c_words_comments)\n",
    "]:\n",
    "    for attr in attr_lookup:\n",
    "        save_path = config.PATH_RES + source + attr + '_dict.json'\n",
    "        if os.path.isfile(save_path):\n",
    "            print(f'\\nSkipping {attr} for {source} since it is already done.')\n",
    "            continue\n",
    "        else:\n",
    "            print(f'\\nDoing {attr} for {source}')\n",
    "        \n",
    "        # create collection and save it\n",
    "        col = init_collection(df, attr, corpus)\n",
    "        col = populate_collection(col, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create collections for communities for both wowpedia pages and wowhead comments\n",
    "for source, corpus, path_words in [\n",
    "    ('wowpedia/', c_words_wiki, config.PATH_WORDS), \n",
    "    ('wowhead/', c_words_comments, config.PATH_COMMENTS_WORDS)\n",
    "]:  \n",
    "    print(f'Computing collections for communities for {source}')\n",
    "    col = {}\n",
    "    save_path = config.PATH_RES + source + 'Louvain_dict.json'\n",
    "    if os.path.isfile(save_path):\n",
    "        continue\n",
    "\n",
    "    for i, names in enumerate(communities): \n",
    "        paths = [\n",
    "            path_words + n.replace(' ', '_') + '.txt' \n",
    "            for n in names\n",
    "        ]\n",
    "        # save text for community\n",
    "        col[i] = {'text': nltk.Text(corpus.words(paths))}\n",
    "    \n",
    "    # create collection and save it\n",
    "    col = populate_collection(col, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Words\n",
    "Inspect top 5 words according to tf-idf for each attribute split and for the different communities by Louvain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top words for attributes\n",
    "for source in ['wowpedia/', 'wowhead/']:\n",
    "    print(f\"\\n\\nFor {source}\")\n",
    "    for attr in attr_lookup:\n",
    "        print(f'\\nTop 5 for attribute {attr}')\n",
    "        col = pickle.load(open(config.PATH_RES + source + attr + '_dict.json', 'rb'))\n",
    "        for split, _ in attr_lookup[attr]:\n",
    "            words = col[split]['words']\n",
    "            tfidf = col[split]['tf'] * col[split]['idf']\n",
    "            top_5 = ', '.join(words[np.argsort(tfidf)[::-1]][:5])\n",
    "            print(f'\\t{split}: {top_5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top words per community\n",
    "for source in ['wowpedia/', 'wowhead/']:\n",
    "    print(f\"\\n\\nFor {source}\")\n",
    "    print(f'Top 5 words for each community')\n",
    "    col = pickle.load(open(config.PATH_RES + source + 'Louvain_dict.json', 'rb'))\n",
    "    for i, com_name in enumerate(com_names):\n",
    "        print(f'\\n\"{com_name}\"')\n",
    "        words = col[i]['words']\n",
    "        tfidf = col[i]['tf'] * col[i]['idf']\n",
    "        top_5 = ', '.join(words[np.argsort(tfidf)[::-1]][:5])\n",
    "        print(top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c8b715ef500c155f7e623c10e2a2705ee7484473bce4c48cafd25806eafb59b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
