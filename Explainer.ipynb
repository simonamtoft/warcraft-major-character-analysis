{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook for Warcraft Major Character Analysis\n",
    "\n",
    "Resulting website: [youngpenguin.github.io/WOW/](https://youngpenguin.github.io/WOW/)\n",
    "\n",
    "Throwback to YouTube video: [Social Graphs Project A](https://www.youtube.com/watch?v=JJx5f5nSYfs&ab_channel=SimonPedersen)\n",
    "\n",
    "GitHub repos: \n",
    "- Analysis repo [github.com/simonamtoft/warcraft-major-character-analysis](https://github.com/simonamtoft/warcraft-major-character-analysis)\n",
    "- Website repo [github.com/YoungPenguin/WOW](https://youngpenguin.github.io/WOW/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "- What is your dataset?\n",
    "- Why did you choose this/these particular dataset(s)?\n",
    "- What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation for this project is to explore characteristics of the main characters in Warcraft, since many people have played the game, but without paying attention to the character traits and stories of the different characters. In the Warcraft universe, the plot revolves around two competing factions, the Horde and the Alliance, which are in constant conflict with eachother. We would thus like to compare how these factions differ. Moreover, we would like to see whether the users experience matches up with the traits of these characters, evaluated by examining user comments.\n",
    "\n",
    "Our dataset consists of data from [https://wowpedia.fandom.com/](https://wowpedia.fandom.com/), and [https://www.wowhead.com/](https://www.wowhead.com/). Since the Warcraft universe is rather large, we've chosen to investigate a subset of these, by analysing the network of [the Major Characters in Warcraft wowpedia](https://wowpedia.fandom.com/wiki/Major_characters), along with their corresponding threads on [wowhead](https://www.wowhead.com/), which contains a lot of user comments. \n",
    "\n",
    "The goal of the end user's experience is for the user to be able to easily see the characteristics of some of the main groupings of Warcraft characters, by evaluating their connections to eachother and investigating the most describing words for groups of characters.\n",
    "For maximum enjoyment we've tried to develop the website such that it is enjoyable on any device, be it a home computer with a large monitior, or a small smart phone with a tilted screen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics\n",
    "Let's understand the dataset better\n",
    "- Write about your choices in data cleaning and preprocessing\n",
    "- Write a short section that discusses the dataset stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform text and network analysis on the chosen data, a lot of data cleaning and preprocessing had to take place.\n",
    "\n",
    "The primary tools that have been used to get the data are \n",
    "- `BeautifulSoup` and `requests` for wowhead comments (see `download_character_comments.py`)\n",
    "- `urllib` for wowpedia pages (see `api.py`)\n",
    "\n",
    "For the wowpedia pages, both the raw and clean pages have been downloaded (see `/data/wow_chars/` and `/data/wow_chars_clean/`), while for wowhead the comments were downloaded and stored along with all its metadata, and later cleaned. For cleaning and preprocessing of the data, the primary tools that have been used are the text analysis library `nltk` and the regular expressions library `re`. The cleaning, preprocessing and downloading from websites will be described more in-depth in the next section under Tool 1.\n",
    "\n",
    "The data can be split into the following subdivisions:\n",
    "1. Wowhead comments (~700 files, 31MB)\n",
    "    1. Raw .njson files containing every comment on the wowhead character pages along with some metadata, like dates. (see `download_character_comments.py`)\n",
    "        1. Note: For some characters, multiple pages are present in which we got comments from all of them.\n",
    "    2. Processed .txt files, which consists of the comments from the raw files without any metadata (see `comments_clean.py`)\n",
    "    3. Words .txt files, which based on the processed .txt files have had stopwords removed, text has been tokenized and every word has been lemmatized (see `comments_to_words.py`)\n",
    "2. Wowpedia pages (~1000 files, 19MB)\n",
    "    1. Raw .txt files containing the entire wowpedia character page (see `download_character_pages.py`)\n",
    "    2. Clean .txt files containing the clean version of the wowpedia character page (see `download_character_pages_clean.py`)\n",
    "    3. Quotes .txt files, which based on the raw .txt files consists of all the quotes from the characters quote section on its wowpedia page (see `extract_character_quotes.py`)\n",
    "    4. Words .txt files, which based on the clean .txt files have had stopwords removed, text has been tokenized and every word has been lemmatized (see `pages_to_words.py`)\n",
    "\n",
    "The resulting dataset consists of ~1700 files (totalling 50 MB).\n",
    "\n",
    "The resulting network graph has 261 nodes, index by the character name, and 4009 edges, where each node has the following four attributes `faction`, `gender`, `race` and `status` (see `Graph Analysis.ipynb`). The creation of the graph will be described in tool 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, Theory and Analysis\n",
    "Describe the process of theory to insight\n",
    "- Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "- Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "- How did you use the tools to understand your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 1: Downloading and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Raw and Clean Wowpedia pages\n",
    "In order to start off the project, the first thing that had to be done was to download all the raw wowpedia pages for each of the [major characters](https://wowpedia.fandom.com/wiki/Major_characters). In order to extract links from the wowpedia pages to create a network graph, a single regex was constructed, which finds links from double brackets: `\\[\\[(.*?)(?:[\\|#].*?)?\\]\\]`. Additionally, python functions were made to generate queries and getting responses from webpages using said queries (see `api.py`). In this process, it is important to remember to keep special characters when constructing the query, which was done with `quote_plus` from `urllib`, and also to decode the response from the webpage with `utf-8` format and make sure it is json serializable with `json.loads`. \n",
    "\n",
    "Then, we could create a list of the names of each of the major characters by extracing the links from the [major characters page](https://wowpedia.fandom.com/wiki/Major_characters), which was done by first querying the webpage with `urllib.request` and then extracting the page content from the response. In order to extract the character names from the raw wowpedia text response, it was first cleaned by:\n",
    "1. removing everything before the list of characters by splitting on `\"==[[Races]]==\"`\n",
    "2. removing all headers with `re.sub(r'==(.+)==', '', txt)` \n",
    "3. removing last part of page that links to another collection of main characters by splitting on `'{{Main characters}}'`\n",
    "\n",
    "Then, the character names were found by looking for all linking patterns (defined earlier) remaining on the page. For each of these characters, both their raw and clean wowpedia pages were downloaded (see `download_character_pages.py` and `download_character_pages_clean.py`). From the clean pages, we additionally removed headers and excess newline and space chars before storing them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing the Wowpedia Pages\n",
    "The downloaded raw wowpedia pages (`/data/wow_chars/`) was used for two different purposes. \n",
    "1. To create the network graph\n",
    "    - Consisting of attributes `gender`, `race`, `faction` and `status`.\n",
    "        - The attributes were extracted from the pages by finding the Npcbox with `wikitextparser`.\n",
    "    - Each node is a character, indexed by the name.\n",
    "    - Edges are directed edges describing links between wowpedia character pages.\n",
    "        - A directed edge was added if one characters page linked to another. \n",
    "        - Used the linking pattern with double brackets described earlier.\n",
    "    - See `create_wow_graph.py`.  \n",
    "2. Extracting character quotes for sentiment analysis\n",
    "    - Some character pages has a quotes section, listing quotes from the pages character.\n",
    "    - The quotes were extracted by:\n",
    "        1. Extracting the quotes section \n",
    "            - First everything before the quotes section was removed by finding the quotes header with the regex `[q|Q]uotes ?==`\n",
    "                - It searches for a header that containes the word quotes at the end (some quote headers had another word before.)\n",
    "            - Secondly, the next section was found and removed with the following regex `\\s==([^=]+)==\\s`, which finds a header.\n",
    "            - Note there were multiple levels of headers, e.g. with three equal signs, which are subheaders we want to keep in the quotes section and thus the regex excludes those.\n",
    "        2. Removing gallary section.\n",
    "            - Contained in some quotes sections.\n",
    "            - Removed by the regex `<gallery>[^><]+<\\/gallery>`\n",
    "        3. Removing references, could be images etc., with three different regex:\n",
    "            - `<ref>[^<]+<\\/ref>`\n",
    "            - `<ref name=[\\w\\d \\-\"\\'().,]+>.+<\\/ref>`\n",
    "            - `<ref name=[\\w\\d \\-\"\\'().,]+\\/>`\n",
    "        4. Headers and section headers are removed by the following two regex:\n",
    "            - `==(.+)==`, finds headers which consists of two equal signs on every side and text in between.\n",
    "            - `\\{\\{\\w+\\-section\\}\\}`, finds sub quote sections which are in double curly brackets and ends with '-section'.\n",
    "        5. Replace markdown style line breaks, `<br>`, with standard line breaks: `re.sub('</?br>', '\\n', quotes)`.\n",
    "        6. Remove \"{{sic}}\" with the regex `\\{\\{sic\\}\\}`.\n",
    "        7. Remove patch information\n",
    "            - `\\(Patch \\d(.\\d)+\\)`, finds a parenthesis containing the word Patch and patch number with varying number of digits e.g. \"(Patch 4.2.1)\".\n",
    "            - `\\(Removed in ''Patch \\d.\\d.\\d\\w?''\\)`, finds a parenthesis describing something that was removed during a specific patch, e.g. \"(Removed in ''Patch 2.1.2a'')\".\n",
    "        8. Extract text from an url link and replace it \n",
    "            - regex: `\\[(https?:\\/\\/[\\w\\/.]+)(.+)\\]`\n",
    "            - replace entire found string only by the second group in the regex `(.+)`.\n",
    "        9. Remove the chars `*` and `:` from the text\n",
    "        10. Remove linking patterns in double curly brackets\n",
    "            - regex: `\\{\\{[M|m]ain\\|([\\w ()\\/\\'\\!\\,\\?\\-\\.]+)(?:#[\\w \\'\\-]+)?\\}\\}`\n",
    "        11. Remove markdown in-line comments with style `<!-- text -->`.\n",
    "            - regex: `<!--[^<]+-->`\n",
    "        12. Extract words from double bracket linkings\n",
    "            - regex: `\\[\\[([\\w '\\(\\)\\-,.\\?\\!#]+\\|(?:\\w+\\|)?)?([\\w\\d ',.\\-\\?()\\!]+)\\]\\]`\n",
    "            - The regex matches the linking pattern and hereafter the match is substituted by the last group of the regex `([\\w\\d ',.\\-\\?()\\!]+)`.\n",
    "        13. Read the remaining text line by line, since there is some weird formatting that is easilier handled this way.\n",
    "            1. Remove lines containing \".jpg\" or \".png\", since it is residue from non-perfect regex.\n",
    "            2. Extract quotes from text with character name with format \"{{text|--|person|quote}}\"\n",
    "                - regex: `\\{\\{(?:[t|T]ext)\\|(?:[s|S]ay|[y|Y]ell|[w|W]hisper)\\|([^|]+)\\|([^\\|\\{]+)\\}\\}`\n",
    "                - The first capture group will be the character, and the second will be the quote. \n",
    "                - The character could be a link to a character, and if so extract the character name from the link.\n",
    "                    - regex: `\\{\\{(npc|NPC)\\|\\|([\\w \\-\\']+)(\\|\\|[\\w \\-\\']+)?\\}\\}` \n",
    "                    - replace whatever is found by the regex, with the last of the groups.\n",
    "                - If the resulting character is not the same character as we are extracting quotes for, discard the line, by setting it to an empty string.\n",
    "            3. Extract quotes from pattern \"{{--|person|quote}}\"\n",
    "                - regex: `\\{\\{(npc|NPC)\\|\\|([\\w \\-\\']+)(\\|\\|[\\w \\-\\']+)?\\}\\}`\n",
    "                - If the resulting character is not the same character as we are extracting quotes for, discard the line, by setting it to an empty string.\n",
    "            4. Extract quote from a text quote pattern {{text|--|quote}}\n",
    "                - regex: `\\{\\{(?:text|Text)\\|(?:[s|S]ay|[y|Y]ell|[w|W]hisper)\\|(.+)\\}\\}`\n",
    "                - simply replace the line by the last group of the regex that captures the quote `(.+)`.\n",
    "            5. Find characters in quotations \"'''character name'''\"\n",
    "                - regex: `\\'\\'\\'([\\w ]+)\\'\\'\\'`\n",
    "                - If the resulting character is not the same character as we are extracting quotes for, discard the line, by setting it to an empty string, otherwise replace the found regex and keep rest of line.\n",
    "            6. Extract quote from gossip pattern \"{{gossip|quote}}\"\n",
    "                - regex: `\\{\\{([G|g]ossip\\|)(.+)\\}\\}`\n",
    "                - replace the regex matching with just the last group.\n",
    "            7. Remove excess spaces from the line\n",
    "            8. Remove references to unit quotes\n",
    "            9. Remove residue quote headers\n",
    "            10. Handle nested links\n",
    "                - regex: `\\{\\{([^|{]+\\|(?:[^|]+\\|)?)(.+)\\}\\}`\n",
    "                - Again replace by last group of regex if found.\n",
    "            11. Remove actions with pattern `<action>`\n",
    "                - `re.sub(r'<[\\w .\\/\\',\\?]+>', '', line)`\n",
    "            12. Remove apostrophes that appear together in groups of 2 or larger.\n",
    "                - `re.sub(r\"''+\", '', line)`\n",
    "            13. Remove everything in double curly brackets {{}} or parenthesis ()\n",
    "                - After all the other cleaning, this will not be quotes.\n",
    "                - `re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', \" \", line)`\n",
    "                - `re.sub(r'\\([^\\(\\)]+\\)', \" \", line)`\n",
    "            14. Remove residue from a non-perfect link regex, where line starts with \"word=\"\n",
    "                - `re.sub(r'\\w+=', ' ', line)`\n",
    "            15. Remove specific characters at the end\n",
    "                - Includes: `\", “, —, [, ], ;, -`\n",
    "            16. Remove excess spaces: `re.sub(r'[ ]+', ' ', lines).strip()`\n",
    "    - See `extract_character_quotes.py`.\n",
    "\n",
    "\n",
    "The downloaded clean wowpedia pages (`/data/wow_chars_clean/`) were further processed in order to later perform text analysis on.\n",
    "- For each clean character page text file, the following processing was done\n",
    "    1. Clean character file was read in as text using encoding `utf-8`.\n",
    "    2. Small cleaning was done by removing patch information\n",
    "        - regex: `Patch \\d.\\d.\\d \\(\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d\\)\\:`\n",
    "    3. Character names were stripped from the text (not including nicknames).\n",
    "    4. Remove apostrophes, since a lot of specific Warcraft words contain these, and we want to keep the meaning of those words.\n",
    "    4. Entire text was turned to lowercase with `.lower()`.\n",
    "    4. Text was turned into tokens with the `WordPunctTokenizer` from `nltk`. \n",
    "    5. Tokens that are stopwords and non-word tokens were removed, resulting in a list of words.\n",
    "    6. Every word were lemmatized using the `WordNetLemmatizer` from `nltk`.\n",
    "    7. Finally, the words were written to a text file using encoding `utf-8`. \n",
    "- See `pages_to_words.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wowhead comments\n",
    "The goal here was to find some augmenting data from Wowhead users for the major characters we extracted from wowpedia. It became apparent that multiple NPCs exist in-game for every character (for technical reasons, etc.), and as a consequence of this, there are multiple Wowhead NPC pages for every character.\n",
    "\n",
    "By hopping around on Wowhead for a bit, we discovered that they have a search page for NPCs which allowed filtering. Thus, we quickly set up a procedure for calling this endpoint with the filter `Has comments` set to `Yes`. Additionally, the search page seemed to fuzzily match the names from the Wikipage characters, but it was not completely resilient to discrepancies (e.g. `Thoras Trollbane` will find some pages, but `Thoras Trollbaen` will not. The search results were embedded in the page as `<div>` elements with a certain *id* attribute. This is where `beautifulsoup` came in handy. It enables you to parse HTML text data into a virtual DOM structure, which allows for more resilient scraping than pure regular expressions can provide. After some DOM and text manipulation, the links to the NPC pages are now extracted for every character.\n",
    "\n",
    "For every NPC page on Wowhead, we needed to scrape the user comments. Luckily, these seemed to be embedded in the NPC pages themselves in a certain `<script>` element. Conveniently, the comment data is already formatted as JSON and can be directly extracted and saved. We save the comments a `.njson` format, which is simply newline-delimited JSON (i.e. every line of the file parses as its own JSON object).\n",
    "\n",
    "The entire procedure of taking Wikipage character names and outputting Wowhead comments is defined in the script `download_character_comments.py`. We also added a timeout to the script to avoid making too many requests to Wowhead and getting blacklisted.\n",
    "\n",
    "\n",
    "The comment text data needed to be cleaned before we could use it for analysis, however. Thus, we created a few regular expressions to help clean up the comment texts:\n",
    "* `re.sub(r\"\\[.+?\\]\", \"\", t)`\n",
    "    * For instance, the comments would often include hyperlinks to other Wowhead pages, e.g. `\"You need to complete [url=http://www.wowhead.com/?quest=10588]Cipher of Damnation[/url] first.\"`. This regular expression finds pairs of brackets `[` and `]` with 1 or more character between them, and matches them lazily (i.e. as few chars as possible). Thus, the result of the example would be  `\"You need to complete Cipher of Damnation first.\"`\n",
    "\n",
    "* `re.sub(r\"(\\\\r|\\\\n|\\\\t)+\", \" \", t)`\n",
    "    * Users also seemed to write out their comments using several lines. This means that a lot of carriage returns, newlines were present in the raw texts, e.g. `\"I say this guy becomes boss lvl.\\r\\n\\r\\nAfter experiencing the ...\"`. These sequences of spacing characters could be replaced with a simple space character. As such, we created a regular expression which finds all sequences of 1 or more consecutive \\r, \\n or \\t characters and replaces them with a single space character.\n",
    "    \n",
    "* `re.findall(r\"(.+?(?:[!.?]+|$))(?:\\s|$)\", t)`\n",
    "    * Depending on how the text is to be analyzed, it can be useful to split a text blob into its individual sentences. The above expression lazily finds all sequences of characters that are followed by 1 or more sentence delimiter characters (i.e. . or !) or the end of sequence, which are subsequently followed by either a space or the end of the sequence. It seems to work better for our use case than `sent_tokenize` from the `nltk` package.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 2: Network Analysis\n",
    "All the analysis described in this section is present in the `Graph Analysis.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Visualizations, Character Connections and Node Attributes\n",
    "We want to investigate how the different characters connect with each other, and which of these characters play a major role in Warcraft by examining how important they are to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the Warcraft network, we have visualized it using the ForceAtlas algorithm, while coloring nodes according to their `Faction` attribute (blue for Alliance, red for Horde and gray for Neutral). Additionally, nodes are scaled according to node degree and edges are colored according to the relationship between the connected nodes, such that the edge is blue between two alliance characters, red between two horde characters, green between a horde and alliance character and gray if one of the connecting nodes is a neutral node.\n",
    "\n",
    "The resulting network has 261 nodes and 4009 links, where Thrall is the most connected character with an in-degree of 103 and out-degree of 84, which is the big red node in the bottom middel.\n",
    "\n",
    "![Network plot](visualizations\\network.png)\n",
    "\n",
    "In order to see which characters are the most important out of the major characters, we inspect the top 5 most connected Alliance, Horde and Neutral characters, which are listed in the table below along with their in- and out-degree.\n",
    "\n",
    "|Alliance| (in, out) | |Horde | (in, out) ||Neutral | (in, out) |\n",
    "|---------------------------|----:|-:|--------------------|----:|-:|--------------------|----:|\n",
    "|\tJaina Proudmoore| 73, 72||\tThrall| 103, 84||\tSylvanas Windrunner| 79, 66|\n",
    "|\tAnduin Wrynn| 68, 63||\tLor'themar Theron| 37, 47||\tDeathwing| 69, 53|\n",
    "|\tVarian Wrynn| 60, 66||\tBaine Bloodhoof|44, 39||\tLich King| 76, 44|\n",
    "|\tKhadgar| 65, 55||\tVol'jin|42, 38||\tGarrosh Hellscream| 63, 40|\n",
    "|\tMalfurion Stormrage| 54, 46||\tVarok Saurfang|39, 36||\tArthas Menethil| 56, 41|\n",
    "\n",
    "As expected, the most connected alliance characters include the current and late kings of Stormwind (Anduin and Varian Wrynn). For the horde characters, we see Thrall, which has been the warchief of Orgrimmar and is probably one of the most known character in Warcraft, and Vol'jin who has also been the warchief of Orgrimmar. Finally, for the Neutral characters we see three end game bosses (Deathwing, Lich King, who was previously known as Arthas Menethil, and Garrosh Hellscream), along with one of the newest boss characters, Sylvanas Windrunner, who used to be a part of the horde faction.\n",
    "\n",
    "\n",
    "We can also inspect the distribution of the node attributes in the network by creating a bar plot for each. From this bar plot we see that genders, faction and status is nicely divided into a few groupings, however the race distribution is populated with a lot of different races with a few occurences apart from the main character races such as Human, Orc and Blood Elf.\n",
    "\n",
    "![Attribute plot](visualizations\\attribute_distributions.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree distribution and random network comparison\n",
    "\n",
    "Since random network theory makes a number of quantitative predictions we can taking advantage of when comparing the warcraft network to a random network. A random network has a binomial distribution, well approximated by a Poisson distribution. the Poisson distribution fails to capture the degree distribution of real networks. \n",
    "To do this, we can create random networks with the same number of nodes and edges as the warcraft network, and plot them against each other:\n",
    "\n",
    "![degree_distributions](./visualizations/degree_distributions.png)\n",
    "\n",
    "\n",
    "As expected, it is immediately apparent that the two networks are significantly different, since their degree distributions are not similar. The degree distribution in a random network follows a Poisson distribution. In contrast, the degree distribution of the Warcraft network is closer to a scale-free distribution. \n",
    "\n",
    "To go further into this, we can calculate the exponents of the degree distribution, which is done by the `powerlaw` package. This resulted in exponents of values 3.02, 4.71 and 3.71 for the in-, out- and total-degree distributions respectively. This means that, according to the exponent of the degree distribtuions, the network exists in the random regime or around the critical point. It makes sense that the network would have a tendency to be in the \"Small World\", since there are many more edges than nodes and thus most nodes are very close to each other. By calculating the average shortest path with `average_shortest_path_length` from `networkx`, we see that this value is only 2.4, meaning that you can go from most characters to any other character in just two or three steps in the directed version of the graph! This property is somewhat to be expected for a network consisting only of the Major characters in Warcraft, which is a very small part of the universe. For the undirected graph, this value is only decreased by 0.1 to 2.3, which doesn't change much.\n",
    "\n",
    "References: \n",
    "- [Network Science Chapter 3](http://networksciencebook.com/chapter/3)\n",
    "- [Network Science: Section 4.7](http://networksciencebook.com/chapter/4#degree-exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communities with Louvain\n",
    "\n",
    "In addition to simply dividing the network into communities based on attributes such as Gender or Faction, we created a community partition using the [Louvain algorithm implementation in Python](https://github.com/taynaud/python-louvain). This is done in order to see how the characters are grouped together by the story and game itself (which is captured in the linkings on wowpedia pages).\n",
    "\n",
    "\n",
    "The Louvain algorithm have two main steps that are repeated iteratively untill maximum modularity is reached. We consider a weighted network of N nodes where each node, *i*, is initially assigned to a different community.\n",
    "1. for each node *i*, calculate the gain in modularity\n",
    "    - Node *i* is moved to the community which has the largest modular positive gain, \n",
    "        - if there is no positive gain found, node *i* will stay in the original community\n",
    "    - The modularity change is calculated with the following formula:\n",
    "        - $ \\Delta M=\\left [ \\frac{\\sum _{in}+2k_{i,in}}{2W}-\\left (\\frac{\\sum _{tot}+k_i}{2W}  \\right )^2 \\right ]-\\left [ \\frac{\\sum _{in}}{2W}-\\left ( \\frac{\\sum _{tot}}{2W} \\right )^2 -\\left ( \\frac{k_i}{2W} \\right )^2 \\right ] \\texttt{(9.58)}$\n",
    "        - the formula essentially looks for communities by optimizing modularity locally.\n",
    "2. constructs a new network from the communities identified in the first step\n",
    "    - where the weight of the links between nodes in the new network are the sum of the weights of the links between the nodes in the community.\n",
    "\n",
    "Using this community seperation algorithm on the network graph resulted in seven communities, where the top three characters of each community (based on node degrees) were:\n",
    "1. Khadgar, Illidan Stormrage, Velen\n",
    "2. Deathwing, Sargeras, Yogg-Saron\n",
    "3. Sylvanas Windrunner, Lich King, Varian Wrynn\n",
    "4. Malfurion Stormrage, Tyrande Whisperwind, Alexstrasza\n",
    "5. Thrall, Ner'zhul, Orgrim Doomhammer\n",
    "6. Anzu, Terokk, Talon King Ikiss\n",
    "7. Jaina Proudmoore, Anduin Wrynn, Garrosh Hellscream\n",
    "\n",
    "\n",
    "From the top characters in each community, it is not exactly obvious what the commonality is for each community, apart from community 5 and 6. Community 5 captures some of the important Horde characters, while community 6 contains characters affiliated with the Arakkoa race, which are located in Outland in their capital Skettis. The characteristics of these communities will become more apparent when investigating their wordclouds (try to compare different wordclouds for the different communities on the webpage). \n",
    "\n",
    "\n",
    "Reference: [Network Science section 9.12](http://networksciencebook.com/chapter/9#advanced-9a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centrality Measures and Assortativity\n",
    "\n",
    "In order to evaluate whether characters are most connected to other characters of the same faction or gender, we computed degree, betweenness and eigenvector centrality on different partitions of the graph based on different node attributes.\n",
    "\n",
    "This was done using the `networkx` library for each of the partitions made on node attributes `faction`, `gender` and `status`, which resulted in the following values (see `Graph Analysis.ipynb` for calculation):\n",
    "\n",
    "|                        |   | Faction |          |         |   | Gender |        |   | Status   |        |\n",
    "|------------------------|---|---------|----------|---------|---|--------|--------|---|----------|--------|\n",
    "| **Centrality measure** |   | Horde   | Alliance | Neutral |   | Male   | Female |   | Deceased | Alive  |\n",
    "| Degree                 |   | 0.134007| 0.179356 | 0.096958|   |0.114426|0.119036|   | 0.125125 |0.102458|\n",
    "| Betweenness            |   | 0.007062| 0.008802 | 0.004222|   |0.005290|0.005649|   | 0.006439 |0.003929|\n",
    "| Eigenvector            |   | 0.047472| 0.067552 | 0.034995|   |0.042228|0.041210|   | 0.044177 |0.039021|\n",
    "\n",
    "In order to further see how much the characters within factions, genders and alive vs. dead characters have in common according to the network, we calculated the assortativity for creating subgraphs based on these attributes, using `attribute_assortativity_coefficient` from `networkx` which resulted in values 0.1691, 0.0451, and 0.0585 for the faction, gender and status partitions respectively. \n",
    "\n",
    "Comparing these assortativity values and the values for the centrality measures, makes it clear that characters in Warcraft are mainly characterized by their faction instead of their gender or whether they are dead or alive. It makes sense that the status is not that big of a role, since multiple of the deceased characters can still be found in the game as ghosts or by jumping to other timelines.\n",
    "\n",
    "References: \n",
    "- [degree centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.degree_centrality.html) \n",
    "- [betweenness centrality](https://networkx.org/documentation/stable/auto_examples/algorithms/plot_betweenness_centrality.html?highlight=betweenness%20centrality)\n",
    "- [eigenvector centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.eigenvector_centrality.html#networkx.algorithms.centrality.eigenvector_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 3: Character Word Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "In order to investigate which words characterize the different characters, we use the TF-IDF and TC-IDF metrics. These metrics consists of:\n",
    "- TC: Term Count\n",
    "    - This is simply the number of occurences a term `t` has in a document `d`.\n",
    "- TF: Term Frequency\n",
    "    - this is the number of times a term `t` appears in a document `d` out of the total number of terms in `d`.\n",
    "    - can be calculated as the term count divided by the number of words in the text.\n",
    "- IDF: Inverse Document Frequency\n",
    "    - the number of documents `N` divided by the number of documents where `t` is present.\n",
    "\n",
    "TF-IDF (or TC-IDF) can then be used to figure out how important words in a text are. The TF-IDF value is proportional to the frequency of the word in the text, also taking into account if the specific words are specific to that said text. The score for a single word is specifically calculated by simply multiplying the term frequency (or term count) with the inverse document frequency, such that $tfidf = tf * idf$ and $tcidf = tc * idf$.\n",
    "\n",
    "When partitioning the characters into communities based on their attributes, we found the most describing words e.g. for Faction to be:\n",
    "- Wowpedia\n",
    "    - Alliance: alleria, naaru, genn, koltira, eredar\n",
    "    - Horde: bwonsamdi, darkspear, tyrathan, cairne, loa\n",
    "- Wowhead\n",
    "\t- Alliance: koltira, skybreaker, lurid, naaru, dreanei\n",
    "\t- Horde: ya, troll, clan, orgrim, da\n",
    "\n",
    "In which we see that these words seem to capture specific aspects of the Alliance or Horde characters. E.g. \"darkspear\" is a tribe of jungle trolls in the game, Naaru is sentient being that are affiliated closely with the Draenei race and \"ya\" is a word often used by trolls.\n",
    "\n",
    "\n",
    "Reference: [tf-idf wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordClouds\n",
    "\n",
    "As mentioned earlier, we would like to capture what describes the different groups of major Warcraft characters the best, which is done gallantly by displaying wordclouds!\n",
    "\n",
    "After calculating TC-IDF values for each of the different graph partitions, it is possible to create strings that can be displayed in wordclouds. These strings are made by creating a string containing each word repeated a number of times determined directly by the TC-IDF value for that word (see `text_helpers.populate_collection`). \n",
    "After computing the wordcloud strings, it is possible to create (fancy looking) wordclouds, capturing the most important words of each different partition of the Warcraft network (see `create_wordclouds.py`).\n",
    "\n",
    "In order to enhance the wordcloud visualizations, we've created masks for each of them (see `/store/masks/`) which captures something about the specific attribute, such that the words are displayed inside each mask filling it out, instead of simply displaying them in a box.\n",
    "\n",
    "Since 13 wordclouds are created for both wowpedia and wowhead, we've decided to just display the Alliance (left) and Horde (right) wordclouds computed from the characters Wowpedia pages below, where we see that the most important words also appear as the biggest words in these wordclouds (see previous section). In order to see the other wordclouds, play around with the dropdown menus in the wordclouds section on the website!\n",
    "\n",
    "Comparing these two wordclouds in particular, we see that it is hard, if not impossible to spot a word that appears in both the Alliance and Horde wordcloud\n",
    "\n",
    "![horde_ally_wordcloud](./visualizations/wowpedia/wc_ally_horde.png)\n",
    "\n",
    "Reference: [WordCloud GitHub](https://github.com/amueller/word_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 4: Sentiment Analysis\n",
    "(The code for this part is in `Sentiment Analysis.iypnb`)\n",
    "\n",
    "The goal for this section is to produce a unidimensional sentiment score for some input text (be it a Wowhead comment, or a Wikipages quote). The idea is that performing this analysis will provide a new perspective on the data in our network.\n",
    "\n",
    "\n",
    "For the sentiment analysis part to be carried out, we needed some text to analyze first. Here, we chose to work with text from the scraped Wowhead comments, and with text from the extracted quotes from the Wikipages.\n",
    "\n",
    "We chose two different methods for performing the sentiment analysis:\n",
    "* [VADER](https://github.com/cjhutto/vaderSentiment)\n",
    "    * VADER is a dictionary- and rule-based approach to evaluating text sentiment. This means that under the hood, VADER has a lookup table which is used to assign sentiment scores to tokens individually. However, it has rules which alter the sentiment vaules based on negations, various punctuations, degree modifiers (e.g. *very* or *somewhat*), slang words, emojis, and much more. Furthermore, it should be noted that VADER is specifically tuned for text from social media. The VADER dictionary (or lexicon, as they call it) consists of about 7500 tokens. VADER produces several dimensions for sentiment, but the `compound` dimensions is stated as being the best unidimensional measure of sentiment (it is described as a \"normalized, weighted composite score\")\n",
    "\n",
    "* [BERT (flairNLP)](https://github.com/flairNLP/flair)\n",
    "    * This approach is based on BERT (which is a deep learning technique for natural language modeling). It works by breaking down the input text into tokens, and assigning an embedding to every token. These embeddings are then combined in a manner which takes context/sequence into account (through *attention* mechanisms) and produces a single embedding for the entire input text. Classification can then be performed on this resulting vector, which turns it into a sentiment score (either positive/negative). What is noteworthy about this approach is its ability to capture long-range dependencies in sequences, and its large vocabulary size (this depends of the specific BERT variant, but is mostly in the scope of ~30K different tokens).\n",
    "\n",
    "\n",
    "The VADER procedure is carried out as follows:\n",
    "1. Break down input text into individual sentences (using previously described regex)\n",
    "2. For every sentence, we pass it to VADER's `polarity_scores(...)` method and retrieve its `compound` score\n",
    "3. Reduce all sentences to a single score by averaging them, and return it\n",
    "\n",
    "\n",
    "The BERT procedure is carried out as follows:\n",
    "1. Pass the input text directly to flair by creating a `flair.data.Sentence(...)` and subsequently calling `.predict(...)` on it\n",
    "2. The prediction output is consists of a confidence in the range [0.5, 1], along with a binary class label {`NEGATIVE`, `POSITIVE`}. The confidence is mapped from [0.5, 1] to [0, 1] by subtracting 0.5 and multiplying the result by 2. It is then multiplied by -1 if the predicted class is `NEGATIVE`\n",
    "3. Return the converted score\n",
    "\n",
    "For both procedures, we reduce the sentiment scores for the individual text entities (Wowhead comments or Wikipage quotes) to a single character-level score by taking the mean of the scores for all comments/quotes that belong to the respective characters.\n",
    "\n",
    "\n",
    "With the two procedures in place, we can look at how the models compare by sampling a couple of comments:\n",
    "*  `Great job Blizzard, Alliance finally have a faction boss to be proud of.`\n",
    "    * BERT 1.00, VADER 0.80\n",
    "    * The two methods seem to pretty clearly agree on this one\n",
    "    \n",
    "* `Rest in Peace, he had such potential as a Warchief, wish Blizz didn't kill him off so quickly.`\n",
    "    * BERT -0.53, VADER 0.87\n",
    "    * The two methods disagree on this one. We find this one a bit difficult to score ourselves, since there can be found arguments for both sides in the case of positive/negative scoring.\n",
    "    \n",
    "* `For Blood DKs, Anti-Magic Shell can be used to negate a stack of Black Scar from Mandible Slam at the moment.`\n",
    "    * BERT 0.78, VADER -0.38\n",
    "    * The methods seem to disagree on this one as well. If we were to assign this one a score, it would be quite neutral. The comment is simply stating a fact in regard to some abilities in the game.\n",
    "    \n",
    "* `Meryl is awesome and it's awesome that we're finally seeing him in-game.`\n",
    "    * BERT 1.00, VADER 0.09\n",
    "    * This example is pretty clearly of positive sentiment. However, it seems that only BERT picks up on this to a significant degree, which seems a bit odd, since VADER does have a lexicon entry for \"awesome\".\n",
    "\n",
    "\n",
    "It seems that the two sentiment analysis methods disagree at times. As such, it would be interesting to see how the VADER sentiment scores compare to the BERT scores on a bigger scale. To do this, we can take gather all the calculated scores for all comments and wikipedia quotes, and plot the following histograms:\n",
    "\n",
    "![Sentiment distributions](visualizations/sentdist.png)\n",
    "\n",
    "It is immediately apparent that the two methods produce significantly different scores for the text data we extracted. The VADER scores are very dense around 0, and get quite shallow towards the tails, with barely any scores when approaching the minimum and maximum of -1 and 1 respectively. Quite contrarily, the BERT scores are very heavily distributed towards the minimum and maximum, with more negative scores than negative in total. Very few BERT scores seem to lie in the middle of the continuum.\n",
    "\n",
    "This difference in score distributions could probably stem from the following facts:\n",
    "* Many out-of-lexicon tokens will lead to a high concentration of 0-scores for the VADER method. The BERT method has a bigger vocabulary, and out-of-vocabulary tokens do not have as big of an impact on this type of model.\n",
    "* The BERT method uses a model which has been trained as a binary classification task. This could lead to a sharp decision boundary of confidence either being 0 or 1.\n",
    "* Neither of the two methods are attuned to terms that arise from a fantasy universe such as the Warcraft universe. Words like \"Dragon\", \"Death\", \"Monster\", and \"Thief\" will affect the sentiment differently when the context of the Warcraft universe\n",
    "\n",
    "It is important to have the mentioned differences in mind when comparing sentiment analysis results for the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 5: Time-Series Sentiment Analysis of Wowhead Comments\n",
    "(The code for this section is defined in `Sentiment Analysis.ipynb`)\n",
    "\n",
    "The comments also include a timestamp of when they were created. This means that we can plot the sentiment of comments as a time series, and try to reveal any potential trends in sentiment scores for characters.\n",
    "\n",
    "We ended up with 4 different parameters for the configuration which controls how the plots are created.\n",
    "* Breakdown\n",
    "    * Groups the characters together by some characteristic. For instance, they can be grouped together by their defined `\"Faction\"` from Wikipages, or their `\"Community\"` which is derived from network characteristics)\n",
    "    * `[\"Faction\", \"Gender\", \"Status\", or \"Community\"]`\n",
    "* Metric\n",
    "    * A switch for which metric is being displayed for the comments. `\"rating\"` is the Wowhead comment rating\n",
    "    * `[\"bert_sentiment\", \"vader_sentiment\", \"rating\"]`\n",
    "* Resample\n",
    "    * Controls the date aggregation. Groups comments together by their date\n",
    "    * `[\"Month\", \"Quarter\", \"Year\"]`\n",
    "* Replies\n",
    "    * Whether or not to include comment replies in the calculations\n",
    "    * `[true, false]`\n",
    "\n",
    "Then we generated time series data for all combinations of parameters and saved it to a JSON file. The website then allows users to control the parameters through input components, which will swap out the data being presented based on which inputs are selected.\n",
    "\n",
    "A sample time series plot is shown below:\n",
    "\n",
    "![Example TS plot](./visualizations/ts-example.png)\n",
    "\n",
    "The above plot is created with a `Faction` breakdown, showing the `bert_sentiment` with `Q` (quarterly) date aggregation and excluding replies. It seems that the sentiment score for alliance and horde characters become quite volatile after around 2015. Also, it seems that alliance characters had high sentiment scores initially in 2006.\n",
    "\n",
    "However, we are lacking some sense of signifiance in the plot. It may be true that the mean sentiment scores are more extreme, but, arguably, we need to know how many comments were included when generating the mean score. If very few comments were included in order to produce a high sentiment score, it becomes a lot less meaningful. To examine this, we created an accompanying time series plot to the one shown before.\n",
    "\n",
    "![TS counts plot](./visualizations/ts-counts.png)\n",
    "\n",
    "And, as suspected, it seems that there are very few comments for alliance and horde characters during the volatile periods. When compared to the amount of neutral character comments, it seems that the amount of alliance and horde character comments is significantly lower.\n",
    "\n",
    "\n",
    "In conclusion, it should be noted that the time series plots lack the ability to illustrate the signifiance of the mean character sentiments. A possible solution could be to add error bars in order to portray either standard deviation, or some other measurement entirely which captures how many comments are in every date bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "Think critically about your creation\n",
    "- What went well?\n",
    "- What is still missing? \n",
    "- What could be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What went well**\n",
    "\n",
    "In general every part of the project turned out quite well. The website ended up being far more interactive than initially expected, and is very easy to play around with - it is far better at comparing different wordclouds or attribute distributions than a plain python notebook, especially considering the graph interactions, where hovering over a node in the network visualization or the bars and points in the different plots provides the user with loads of information.\n",
    "\n",
    "However, there were a lot of issues cleaning the character quotes from the raw wowpedia pages, which would've been nice to avoid. Since it used many regular expressions we also decided not to comment on the functionality of each and every one on them. The resulting character quotes turned out nicely and made it possible to do sentiment of the actual characters instead of their wikipages, which is far from the same, making it possible to do more comparisons of user comments and the characters themselves. \n",
    "\n",
    "Another thing to note is that the network is very small with only around 250 nodes, and therefore the things that can be drawn from the analysis of the connections etc. in the network is quite limited.\n",
    "\n",
    "\n",
    "\n",
    "**Missing**\n",
    "\n",
    "One thing we did not include that we could've included was to investigate collocations and concordance in both the user comments from wowhead and on the wikipages and compared these. Another thing that is somewhat missing is adding some kind of weight of the edges in the network, which would allow us to do [backbone analysis](https://www.pnas.org/content/pnas/106/16/6483.full.pdf) of the network.\n",
    "\n",
    "**Improvements**\n",
    "\n",
    "If we were to improve the current work, there are multiple things that could be done (possibilities are somewhat endless here):\n",
    "- improve character sentiment by adding more character quotes from the wowpedia pages\n",
    "    - Some quote sections include linkings to collections of character quotes, from e.g. Warcraft 2, which could be downloaded and processed.\n",
    "    - Manually find the different quoting pages on wowpedia and process these, adding quotes to major characters. \n",
    "    - Finding quotes from outside wowpedia, e.g. the Warcraft movie script or the different Warcraft books (probably behind paywalls)\n",
    "        - Could lead to time series analysis of charcter sentiment, which could be compared to the sentiment time series of wowhead comments.\n",
    "- expanding the network to not only include the [major character page on wowpedia](https://wowpedia.fandom.com/wiki/Major_characters)\n",
    "- handle character nicknames\n",
    "    - some characters have different nicknames that could also be handled in some way.\n",
    "    - this is currently done when extracting character quotes in `extract_character_quotes.py`.\n",
    "        - could be more elaborate and added for every processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contributions\n",
    "Who did what? You should write (just briefly) which group member was the main responsible for which elements of the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main responsibilities**\n",
    "* Janus Ivert Johansen, s173917\n",
    "    * Setting up the website \n",
    "    * Wordcloud masks\n",
    "    * Network visualization\n",
    "    * Degree Distributions\n",
    "* Lucas Alexander Sørensen, s174461\n",
    "    * Downloading comments from wowhead\n",
    "    * Sentiment Analysis (Bert and VADER)\n",
    "    * Timeseries sentiment\n",
    "* Simon Amtoft Pedersen, s173936\n",
    "    * Downloading & cleaning data\n",
    "    * Extracting character quotes from wowpedia character pages\n",
    "    * Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "    * Wordclouds\n",
    "    * Centrality Measures & Assortativity\n",
    "    * Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
