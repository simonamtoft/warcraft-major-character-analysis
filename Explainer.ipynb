{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook for Warcraft Major Character Analysis\n",
    "\n",
    "Resulting website: [youngpenguin.github.io/WOWenShittyWebsite/](https://youngpenguin.github.io/WOWenShittyWebsite/)\n",
    "\n",
    "GitHub repos: \n",
    "- Analysis repo [github.com/simonamtoft/warcraft-major-character-analysis](https://github.com/simonamtoft/warcraft-major-character-analysis)\n",
    "- Website repo [github.com/YoungPenguin/WOWenShittyWebsite](https://github.com/YoungPenguin/WOWenShittyWebsite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "- What is your dataset?\n",
    "- Why did you choose this/these particular dataset(s)?\n",
    "- What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this project comes from:\n",
    "- Character pages from [wowpedia.fandom.com/wiki/Wowpedia](https://wowpedia.fandom.com/wiki/Wowpedia)\n",
    "    - We only inspect the major characters found on [wowpedia.fandom.com/wiki/Major_characters](https://wowpedia.fandom.com/wiki/Major_characters)\n",
    "- Corresponding character pages from [www.wowhead.com/](https://www.wowhead.com/)\n",
    "\n",
    "\n",
    "We chose the wowpedia data in order to create a network of the major characters in Warcraft as our starting point. We then included user comments from their corresponding character pages from wowhead in order to compare sentiments of users comments of characters to their actual sentiments, along with other text analysis.\n",
    "\n",
    "\n",
    "The goal of the end user's experience is for the user to be able to see the characteristics of groupings of the major characters in Warcraft by different attributes, e.g. comparing wordclouds of Alliance and Horde characters, and seeing what other people connect these characters with (using wowhead comments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics\n",
    "Let's understand the dataset better\n",
    "- Write about your choices in data cleaning and preprocessing\n",
    "- Write a short section that discusses the dataset stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform text and network analysis on the chosen data, a lot of data cleaning and preprocessing had to take place.\n",
    "\n",
    "The primary tools that have been used to get the data are \n",
    "- `BeautifulSoup` and `requests` for wowhead comments (see `download_character_comments.py`)\n",
    "- `urllib` for wowpedia pages (see `api.py`)\n",
    "\n",
    "For the wowpedia pages, both the raw and clean pages have been downloaded (see `/data/wow_chars/` and `/data/wow_chars_clean/`), while for wowhead the comments were downloaded and stored along with all its metadata, and later cleaned.\n",
    "\n",
    "For cleaning and preprocessing of the data, the primary tools that have been used are the text analysis library `nltk` and the regular expressions library `re`. <br> \n",
    "The cleaning, preprocessing and downloading from websites will be described more in-depth in the next section under Tool 1.\n",
    "\n",
    "The data can be split into the following subdivisions:\n",
    "1. Wowhead comments (~700 files, 31MB)\n",
    "    1. Raw .njson files containing every comment on the wowhead character pages along with some metadata, like dates. (see `download_character_comments.py`)\n",
    "        1. Note: For some characters, multiple pages are present in which we got comments from all of them.\n",
    "    2. Processed .txt files, which consists of the comments from the raw files without any metadata (see `comments_clean.py`)\n",
    "    3. Words .txt files, which based on the processed .txt files have had stopwords removed, text has been tokenized and every word has been lemmatized (see `comments_to_words.py`)\n",
    "2. Wowpedia pages (~1000 files, 19MB)\n",
    "    1. Raw .txt files containing the entire wowpedia character page (see `download_character_pages.py`)\n",
    "    2. Clean .txt files containing the clean version of the wowpedia character page (see `download_character_pages_clean.py`)\n",
    "    3. Quotes .txt files, which based on the raw .txt files consists of all the quotes from the characters quote section on its wowpedia page (see `extract_character_quotes.py`)\n",
    "    4. Words .txt files, which based on the clean .txt files have had stopwords removed, text has been tokenized and every word has been lemmatized (see `pages_to_words.py`)\n",
    "\n",
    "The resulting dataset consists of ~1700 files (totalling 50 MB).\n",
    "\n",
    "The resulting network graph has 261 nodes and 4009 edges (see `Graph Analysis.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, Theory and Analysis\n",
    "Describe the process of theory to insight\n",
    "- Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "- Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "- How did you use the tools to understand your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 1: Downloading and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Raw and Clean Wowpedia pages\n",
    "In order to start off the project, the first thing that had to be done was to download all the raw wowpedia pages for each of the [major characters](https://wowpedia.fandom.com/wiki/Major_characters). \n",
    "\n",
    "In order to extract links from the wowpedia pages, a single regex was constructed, which finds links from double brackets: `\\[\\[(.*?)(?:[\\|#].*?)?\\]\\]`.\n",
    "\n",
    "Additionally, python functions were made to generate queries and getting responses from webpages using said queries (see `api.py`). In this process, it is important to remember to keep special characters when constructing the query, which was done with `urllib.parse.quote_plus`, and also to decode the response from the webpage with `utf-8` format and make sure it is json serializable with `json.loads`. \n",
    "\n",
    "Then, the name of each of the major characters were found by extracing the links from the [major characters page](https://wowpedia.fandom.com/wiki/Major_characters), which was done by first querying the webpage with `urllib.request` and then extracting the page content from the response. In order to extract the character names from the raw wowpedia text response, it was first cleaned by:\n",
    "1. remove everything before the list of characters by splitting on `\"==[[Races]]==\"`\n",
    "2. removing all headers with `re.sub(r'==(.+)==', '', txt)` \n",
    "3. remove last part of page that links to another collection of main characters by splitting on `'{{Main characters}}'`\n",
    "\n",
    "Then, the character names were found by looking for all linking patterns (defined earlier) remaining on the page. \n",
    "\n",
    "For each of these characters, both their raw and clean wowpedia pages were downloaded (see `download_character_pages.py` and `download_character_pages_clean.py`). \n",
    "\n",
    "From the clean pages, we additionally removed headers and excess newline and space chars before storing them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing the Wowpedia Pages\n",
    "The downloaded raw wowpedia pages (`/data/wow_chars/`) was used for two different purposes. \n",
    "1. To create the network graph\n",
    "    - Consisting of attributes `gender`, `race`, `faction` and `status`.\n",
    "        - The attributes were extracted from the pages by finding the Npcbox with `wikitextparser`.\n",
    "    - Each node is a character\n",
    "    - Edges are directed edges describing links between wowpedia pages.\n",
    "        - A directed edge was added if one characters page linked to another. \n",
    "        - Used the linking pattern with double brackets described earlier.\n",
    "    - See `create_wow_graph.py`.  \n",
    "2. Extracting character quotes for sentiment analysis\n",
    "    - Some character pages has a quotes section, listing quotes from the pages character.\n",
    "    - The quotes were extracted by:\n",
    "        1. Extracting the quotes section \n",
    "            - First everything before the quotes section was removed by finding the quotes header with the regex `[q|Q]uotes ?==`\n",
    "                - It searches for a header that containes the word quotes at the end (some quote headers had another word before.)\n",
    "            - Secondly, the next section was found and removed with the following regex `\\s==([^=]+)==\\s`, which finds a header.\n",
    "            - Note there were multiple levels of headers, e.g. with three equal signs, which are subheaders we want to keep in the quotes section and thus the regex excludes those.\n",
    "        2. Removing gallary section.\n",
    "            - Contained in some quotes sections.\n",
    "            - Removed by the regex `<gallery>[^><]+<\\/gallery>`\n",
    "        3. Removing references, could be images etc., with three different regex:\n",
    "            - `<ref>[^<]+<\\/ref>`\n",
    "            - `<ref name=[\\w\\d \\-\"\\'().,]+>.+<\\/ref>`\n",
    "            - `<ref name=[\\w\\d \\-\"\\'().,]+\\/>`\n",
    "        4. Headers and section headers are removed by the following two regex:\n",
    "            - `==(.+)==`, finds headers which consists of two equal signs on every side and text in between.\n",
    "            - `\\{\\{\\w+\\-section\\}\\}`, finds sub quote sections which are in double curly brackets and ends with '-section'.\n",
    "        5. Replace markdown style line breaks, `<br>`, with standard line breaks: `re.sub('</?br>', '\\n', quotes)`.\n",
    "        6. Remove \"{{sic}}\" with the regex `\\{\\{sic\\}\\}`.\n",
    "        7. Remove patch information\n",
    "            - `\\(Patch \\d(.\\d)+\\)`, finds a parenthesis containing the word Patch and patch number with varying number of digits e.g. \"(Patch 4.2.1)\".\n",
    "            - `\\(Removed in ''Patch \\d.\\d.\\d\\w?''\\)`, finds a parenthesis describing something that was removed during a specific patch, e.g. \"(Removed in ''Patch 2.1.2a'')\".\n",
    "        8. Extract text from an url link and replace it \n",
    "            - regex: `\\[(https?:\\/\\/[\\w\\/.]+)(.+)\\]`\n",
    "            - replace entire found string only by the second group in the regex `(.+)`.\n",
    "        9. Remove the chars `*` and `:` from the text\n",
    "        10. Remove linking patterns in double curly brackets\n",
    "            - regex: `\\{\\{[M|m]ain\\|([\\w ()\\/\\'\\!\\,\\?\\-\\.]+)(?:#[\\w \\'\\-]+)?\\}\\}`\n",
    "        11. Remove markdown in-line comments with style `<!-- text -->`.\n",
    "            - regex: `<!--[^<]+-->`\n",
    "        12. Extract words from double bracket linkings\n",
    "            - regex: `\\[\\[([\\w '\\(\\)\\-,.\\?\\!#]+\\|(?:\\w+\\|)?)?([\\w\\d ',.\\-\\?()\\!]+)\\]\\]`\n",
    "            - The regex matches the linking pattern and hereafter the match is substituted by the last group of the regex `([\\w\\d ',.\\-\\?()\\!]+)`.\n",
    "        13. Read the remaining text line by line, since there is some weird formatting that is easilier handled this way.\n",
    "            1. Remove lines containing \".jpg\" or \".png\", since it is residue from non-perfect regex.\n",
    "            2. Extract quotes from text with character name with format \"{{text|--|person|quote}}\"\n",
    "                - regex: `\\{\\{(?:[t|T]ext)\\|(?:[s|S]ay|[y|Y]ell|[w|W]hisper)\\|([^|]+)\\|([^\\|\\{]+)\\}\\}`\n",
    "                - The first capture group will be the character, and the second will be the quote. \n",
    "                - The character could be a link to a character, and if so extract the character name from the link.\n",
    "                    - regex: `\\{\\{(npc|NPC)\\|\\|([\\w \\-\\']+)(\\|\\|[\\w \\-\\']+)?\\}\\}` \n",
    "                    - replace whatever is found by the regex, with the last of the groups.\n",
    "                - If the resulting character is not the same character as we are extracting quotes for, discard the line, by setting it to an empty string.\n",
    "            3. Extract quotes from pattern \"{{--|person|quote}}\"\n",
    "                - regex: `\\{\\{(npc|NPC)\\|\\|([\\w \\-\\']+)(\\|\\|[\\w \\-\\']+)?\\}\\}`\n",
    "                - If the resulting character is not the same character as we are extracting quotes for, discard the line, by setting it to an empty string.\n",
    "            4. Extract quote from a text quote pattern {{text|--|quote}}\n",
    "                - regex: `\\{\\{(?:text|Text)\\|(?:[s|S]ay|[y|Y]ell|[w|W]hisper)\\|(.+)\\}\\}`\n",
    "                - simply replace the line by the last group of the regex that captures the quote `(.+)`.\n",
    "            5. Find characters in quotations \"'''character name'''\"\n",
    "                - regex: `\\'\\'\\'([\\w ]+)\\'\\'\\'`\n",
    "                - If the resulting character is not the same character as we are extracting quotes for, discard the line, by setting it to an empty string, otherwise replace the found regex and keep rest of line.\n",
    "            6. Extract quote from gossip pattern \"{{gossip|quote}}\"\n",
    "                - regex: `\\{\\{([G|g]ossip\\|)(.+)\\}\\}`\n",
    "                - replace the regex matching with just the last group.\n",
    "            7. Remove excess spaces from the line\n",
    "            8. Remove references to unit quotes\n",
    "            9. Remove residue quote headers\n",
    "            10. Handle nested links\n",
    "                - regex: `\\{\\{([^|{]+\\|(?:[^|]+\\|)?)(.+)\\}\\}`\n",
    "                - Again replace by last group of regex if found.\n",
    "            11. Remove actions with pattern `<action>`\n",
    "                - `re.sub(r'<[\\w .\\/\\',\\?]+>', '', line)`\n",
    "            12. Remove apostrophes that appear together in groups of 2 or larger.\n",
    "                - `re.sub(r\"''+\", '', line)`\n",
    "            13. Remove everything in double curly brackets {{}} or parenthesis ()\n",
    "                - After all the other cleaning, this will not be quotes.\n",
    "                - `re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', \" \", line)`\n",
    "                - `re.sub(r'\\([^\\(\\)]+\\)', \" \", line)`\n",
    "            14. Remove residue from a non-perfect link regex, where line starts with \"word=\"\n",
    "                - `re.sub(r'\\w+=', ' ', line)`\n",
    "            15. Remove specific characters at the end\n",
    "                - Includes: `\", “, —, [, ], ;, -`\n",
    "            16. Remove excess spaces: `re.sub(r'[ ]+', ' ', lines).strip()`\n",
    "    - See `extract_character_quotes.py`.\n",
    "\n",
    "\n",
    "The downloaded clean wowpedia pages (`/data/wow_chars_clean/`) were further processed in order to later perform text analysis on.\n",
    "- For each clean character page text file, the following processing was done\n",
    "    1. Clean character file was read in as text using encoding `utf-8`.\n",
    "    2. Small cleaning was done by removing patch information\n",
    "        - regex: `Patch \\d.\\d.\\d \\(\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d\\)\\:`\n",
    "    3. Character names were stripped from the text (not including nicknames).\n",
    "    4. Remove apostrophes, since a lot of specific Warcraft words contain these, and we want to keep the meaning of those words.\n",
    "    4. Entire text was turned to lowercase with `.lower()`.\n",
    "    4. Text was turned into tokens with the `WordPunctTokenizer` from `nltk`. \n",
    "    5. Tokens that are stopwords and non-word tokens were removed, resulting in a list of words.\n",
    "    6. Every word were lemmatized using the `WordNetLemmatizer` from `nltk`.\n",
    "    7. Finally, the words were written to a text file using encoding `utf-8`. \n",
    "- See `pages_to_words.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wowhead comments\n",
    "The goal here was to find some augmenting data from Wowhead users for the major characters we extracted from wowpedia. It became apparent that multiple NPCs exist in-game for every character (for technical reasons, etc.), and as a consequence of this, there are multiple Wowhead NPC pages for every character.\n",
    "\n",
    "By hopping around on Wowhead for a bit, we discovered that they have a search page for NPCs which allowed filtering. Thus, we quickly set up a procedure for calling this endpoint with the filter `Has comments` set to `Yes`. Additionally, the search page seemed to fuzzily match the names from the Wikipage characters, but it was not completely resilient to discrepancies (e.g. `Thoras Trollbane` will find some pages, but `Thoras Trollbaen` will not. The search results were embedded in the page as `<div>` elements with a certain *id* attribute. This is where `beautifulsoup` came in handy. It enables you to parse HTML text data into a virtual DOM structure, which allows for more resilient scraping than pure regular expressions can provide. After some DOM and text manipulation, the links to the NPC pages are now extracted for every character.\n",
    "\n",
    "For every NPC page on Wowhead, we needed to scrape the user comments. Luckily, these seemed to be embedded in the NPC pages themselves in a certain `<script>` element. Conveniently, the comment data is already formatted as JSON and can be directly extracted and saved. We save the comments a `.njson` format, which is simply newline-delimited JSON (i.e. every line of the file parses as its own JSON object).\n",
    "\n",
    "The entire procedure of taking Wikipage character names and outputting Wowhead comments is defined in the script `download_character_comments.py`. We also added a timeout to the script to avoid making too many requests to Wowhead and getting blacklisted.\n",
    "\n",
    "\n",
    "The comment text data needed to be cleaned before we could use it for analysis, however. Thus, we created a few regular expressions to help clean up the comment texts:\n",
    "* `re.sub(r\"\\[.+?\\]\", \"\", t)`\n",
    "    * For instance, the comments would often include hyperlinks to other Wowhead pages, e.g. `\"You need to complete [url=http://www.wowhead.com/?quest=10588]Cipher of Damnation[/url] first.\"`. This regular expression finds pairs of brackets `[` and `]` with 1 or more character between them, and matches them lazily (i.e. as few chars as possible). Thus, the result of the example would be  `\"You need to complete Cipher of Damnation first.\"`\n",
    "\n",
    "* `re.sub(r\"(\\\\r|\\\\n|\\\\t)+\", \" \", t)`\n",
    "    * Users also seemed to write out their comments using several lines. This means that a lot of carriage returns, newlines were present in the raw texts, e.g. `\"I say this guy becomes boss lvl.\\r\\n\\r\\nAfter experiencing the ...\"`. These sequences of spacing characters could be replaced with a simple space character. As such, we created a regular expression which finds all sequences of 1 or more consecutive \\r, \\n or \\t characters and replaces them with a single space character.\n",
    "    \n",
    "* `re.findall(r\"(.+?(?:[!.?]+|$))(?:\\s|$)\", t)`\n",
    "    * Depending on how the text is to be analyzed, it can be useful to split a text blob into its individual sentences. The above expression lazily finds all sequences of characters that are followed by 1 or more sentence delimiter characters (i.e. . or !) or the end of sequence, which are subsequently followed by either a space or the end of the sequence. It seems to work better for our use case than `sent_tokenize` from the `nltk` package.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 2: Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree distribution and random network comparison\n",
    "\n",
    "Since random network theory makes a number of quantitative predictions we can taking advantage of when comparing the warcraft network to a random network. A random network has a binomial distribution, well approximated by a Poisson distribution. the Poisson distribution fails to capture the degree distribution of real networks. \n",
    "To do this, we can create random networks with the same number of nodes and edges as the warcraft network, and plot them against each other:\n",
    "\n",
    "![degree_distributions](\\visualizations\\degree_distributions.png)\n",
    "\n",
    "\n",
    "\n",
    "As expected, it is immediately apparent that the two networks are significantly different, since their degree distributions are not similar. The degree distribution in a random network follows a Poisson distribution. In contrast, the degree distribution of the Warcraft network is closer to a scale-free distribution. \n",
    "\n",
    "\n",
    "<!-- **Add stuff about coefficient calculated** -->\n",
    "<!-- the following is from assignment 2 -->\n",
    "<!-- **Answer**: \n",
    "\n",
    "The resulting exponents of the degree distribution is then 2.77 for the in-degree and 6.81 for the out-degree.\n",
    "\n",
    "This means, that the exponent of the in-degree distributions places the network in the *scale-free* regime, while the exponent of the out-degree distribution places the network in the *random* regime (reference: [Network Science: Section 4.7](http://networksciencebook.com/chapter/4#degree-exponent)).  -->\n",
    "\n",
    "\n",
    "\n",
    "Reference: [Network Science Chapter 3](http://networksciencebook.com/chapter/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communities with Louvain\n",
    "In addition to simply dividing the network into communities based on attributes such as Gender or Faction, we created a community partition using the [Louvain algorithm implementation in Python](https://github.com/taynaud/python-louvain). The Louvain algorithm have two main steps that are repeated iteratively untill maximum modularity is reached. We consider a weighted network of N nodes where each node, *i*, is initially assigned to a different community.\n",
    "1. for each node *i*, calculate the gain in modularity\n",
    "    - Node *i* is moved to the community which has the largest modular positive gain, \n",
    "        - if there is no positive gain found, node *i* will stay in the original community\n",
    "    - The modularity change is calculated with the following formula:\n",
    "        - $ \\Delta M=\\left [ \\frac{\\sum _{in}+2k_{i,in}}{2W}-\\left (\\frac{\\sum _{tot}+k_i}{2W}  \\right )^2 \\right ]-\\left [ \\frac{\\sum _{in}}{2W}-\\left ( \\frac{\\sum _{tot}}{2W} \\right )^2 -\\left ( \\frac{k_i}{2W} \\right )^2 \\right ] \\texttt{(9.58)}$\n",
    "        - the formula essentially looks for communities by optimizing modularity locally.\n",
    "2. constructs a new network from the communities identified in the first step\n",
    "    - where the weight of the links between nodes in the new network are the sum of the weights of the links between the nodes in the community.\n",
    "\n",
    "Using this community seperation algorithm on the network graph resulted in seven communities, where the top three characters of each community (based on node degrees) were:\n",
    "1. Khadgar, Illidan Stormrage, Velen\n",
    "2. Deathwing, Sargeras, Yogg-Saron\n",
    "3. Sylvanas Windrunner, Lich King, Varian Wrynn\n",
    "4. Malfurion Stormrage, Tyrande Whisperwind, Alexstrasza\n",
    "5. Thrall, Ner'zhul, Orgrim Doomhammer\n",
    "6. Anzu, Terokk, Talon King Ikiss\n",
    "7. Jaina Proudmoore, Anduin Wrynn, Garrosh Hellscream\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reference: [Network Science section 9.12](http://networksciencebook.com/chapter/9#advanced-9a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centrality Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 3: Wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 4: Sentiment Analysis\n",
    "The goal for this section is to produce a unidimensional sentiment score for some input text (be it a Wowhead comment, or a Wikipages quote). The idea is that performing this analysis will provide a new perspective on the data in our network.\n",
    "\n",
    "\n",
    "For the sentiment analysis part to be carried out, we needed some text to analyze first. Here, we chose to work with text from the scraped Wowhead comments, and with text from the extracted quotes from the Wikipages.\n",
    "\n",
    "We chose two different methods for performing the sentiment analysis:\n",
    "* [VADER](https://github.com/cjhutto/vaderSentiment)\n",
    "    * VADER is a dictionary- and rule-based approach to evaluating text sentiment. This means that under the hood, VADER has a lookup table which is used to assign sentiment scores to tokens individually. However, it has rules which alter the sentiment vaules based on negations, various punctuations, degree modifiers (e.g. *very* or *somewhat*), slang words, emojis, and much more. Furthermore, it should be noted that VADER is specifically tuned for text from social media. The VADER dictionary (or lexicon, as they call it) consists of about 7500 tokens. VADER produces several dimensions for sentiment, but the `compound` dimensions is stated as being the best unidimensional measure of sentiment (it is described as a \"normalized, weighted composite score\")\n",
    "\n",
    "* [BERT (flairNLP)](https://github.com/flairNLP/flair)\n",
    "    * This approach is based on BERT (which is a deep learning technique for natural language modeling). It works by breaking down the input text into tokens, and assigning an embedding to every token. These embeddings are then combined in a manner which takes context/sequence into account (through *attention* mechanisms) and produces a single embedding for the entire input text. Classification can then be performed on this resulting vector, which turns it into a sentiment score (either positive/negative). What is noteworthy about this approach is its ability to capture long-range dependencies in sequences, and its large vocabulary size (this depends of the specific BERT variant, but is mostly in the scope of ~30K different tokens).\n",
    "\n",
    "\n",
    "The VADER procedure is carried out as follows:\n",
    "1. Break down input text into individual sentences (using previously described regex)\n",
    "2. For every sentence, we pass it to VADER's `polarity_scores(...)` method and retrieve its `compound` score\n",
    "3. Reduce all sentences to a single score by averaging them, and return it\n",
    "\n",
    "\n",
    "The BERT procedure is carried out as follows:\n",
    "1. Pass the input text directly to flair by creating a `flair.data.Sentence(...)` and subsequently calling `.predict(...)` on it\n",
    "2. The output is converted from two-class [0,1] scores to a single continuum ranging from [-1, 1] by multiplying the class score by -1 if the predicted class is `NEGATIVE`\n",
    "3. Return the converted score\n",
    "\n",
    "With the two procedures in place, we can look at how the models compare for a couple of examples:\n",
    "* Example 1\n",
    "* Example 2\n",
    "* Example 3\n",
    "\n",
    "\n",
    "At this point, it would be interesting to see how the VADER sentiment scores compare to the BERT scores on a bigger scale. To do this, we can take the two scores for all comments and wikipedia quotes, and plot the following histograms:\n",
    "\n",
    "![Sentiment distributions](visualizations/sentdist.png)\n",
    "\n",
    "It is immediately apparent that the two methods produce significantly different scores for the text data we extracted. The VADER scores are very dense around 0, and very shallow tails, with barely any scores when approaching the minimum and maximum of -1 and 1 respectively. On the other hand, the BERT scores are very heavily distributed towards the minimum and maximum, with **more negative scores than negative in total.**  <!-- det her lyder mærkeligt -->\n",
    "\n",
    "This difference in score distributions could probably stem from the following facts:\n",
    "* Many out-of-lexicon tokens will lead to a high concentration of 0-scores for the VADER method. The BERT method has a bigger vocabulary, and out-of-vocabulary tokens do not have as big of an impact on this type of model.\n",
    "* The BERT method uses a model which has been trained as a binary classification task. This could lead to a sharp decision boundary of confidence either being 0 or 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 5: Time-Series Sentiment Analysis of Wowhead Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "Think critically about your creation\n",
    "- What went well?\n",
    "- What is still missing? \n",
    "- What could be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What went well**\n",
    "\n",
    "\n",
    "**Missing**\n",
    "\n",
    "\n",
    "**Improvements**\n",
    "\n",
    "If we were to improve the work, there are multiple things that could be done (possibilities are somewhat endless):\n",
    "- improve character sentiment by adding more character quotes from the wowpedia pages\n",
    "    - Some quote sections include linkings to collections of character quotes, from e.g. Warcraft 2, which could be downloaded and processed.\n",
    "    - Manually find the different quoting pages on wowpedia and process these, adding quotes to major characters. \n",
    "    - Finding quotes from outside wowpedia, e.g. the Warcraft movie script or the different Warcraft books (probably behind paywalls)\n",
    "        - Could lead to time series analysis of charcter sentiment, which could be compared to the sentiment time series of wowhead comments.\n",
    "- expanding the network to not only include the [major character page on wowpedia](https://wowpedia.fandom.com/wiki/Major_characters)\n",
    "- handle character nicknames\n",
    "    - some characters have different nicknames that could also be handled in some way.\n",
    "    - this is currently done when extracting character quotes in `extract_character_quotes.py`.\n",
    "        - could be more elaborate and added for every processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contributions\n",
    "Who did what? You should write (just briefly) which group member was the main responsible for which elements of the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main responsibilities**\n",
    "* Janus Ivert Johansen, s173917\n",
    "    * Setting up the website \n",
    "    * Wordcloud masks\n",
    "    * Network graph visualization\n",
    "* Lucas Alexander Sørensen, s174461\n",
    "    * Downloading comments from wowhead\n",
    "    * Sentiment Analysis (Bert and VADER)\n",
    "    * Timeseries sentiment\n",
    "* Simon Amtoft Pedersen, s173936\n",
    "    * Downloading & cleaning data\n",
    "    * Extracting character quotes from wowpedia character pages\n",
    "    * Wordcloud computations (tf-idf etc.)\n",
    "    * Compute network measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
