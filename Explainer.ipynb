{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook for Warcraft Major Character Analysis\n",
    "\n",
    "Resulting website: [youngpenguin.github.io/WOWenShittyWebsite/](https://youngpenguin.github.io/WOWenShittyWebsite/)\n",
    "\n",
    "GitHub repos: \n",
    "- Analysis repo [github.com/simonamtoft/warcraft-major-character-analysis](https://github.com/simonamtoft/warcraft-major-character-analysis)\n",
    "- Website repo [github.com/YoungPenguin/WOWenShittyWebsite](https://github.com/YoungPenguin/WOWenShittyWebsite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "- What is your dataset?\n",
    "- Why did you choose this/these particular dataset(s)?\n",
    "- What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this project comes from:\n",
    "- Character pages from [wowpedia.fandom.com/wiki/Wowpedia](https://wowpedia.fandom.com/wiki/Wowpedia)\n",
    "    - We only inspect the major characters found on [wowpedia.fandom.com/wiki/Major_characters](https://wowpedia.fandom.com/wiki/Major_characters)\n",
    "- Corresponding character pages from [www.wowhead.com/](https://www.wowhead.com/)\n",
    "\n",
    "\n",
    "We chose the wowpedia data in order to create a network of the major characters in Warcraft as our starting point. We then included user comments from their corresponding character pages from wowhead in order to compare sentiments of users comments of characters to their actual sentiments, along with other text analysis.\n",
    "\n",
    "\n",
    "The goal of the end user's experience is for the user to be able to see the characteristics of groupings of the major characters in Warcraft by different attributes, e.g. comparing wordclouds of Alliance and Horde characters, and seeing what other people connect these characters with (using wowhead comments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics\n",
    "Let's understand the dataset better\n",
    "- Write about your choices in data cleaning and preprocessing\n",
    "- Write a short section that discusses the dataset stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform text and network analysis on the chosen data, a lot of data cleaning and preprocessing had to take place.\n",
    "\n",
    "The primary tools that have been used to get the data are \n",
    "- `BeautifulSoup` and `requests` for wowhead comments (see `download_character_comments.py`)\n",
    "- `urllib` for wowpedia pages (see `api.py`)\n",
    "\n",
    "For the wowpedia pages, both the raw and clean pages have been downloaded (see `/data/wow_chars/` and `/data/wow_chars_clean/`), while for wowhead the comments were downloaded and stored along with all its metadata, and later cleaned.\n",
    "\n",
    "For cleaning and preprocessing of the data, the primary tools that have been used are the text analysis library `nltk` and the regular expressions library `re`. <br> \n",
    "The cleaning, preprocessing and downloading from websites will be described more in-depth in the next section under Tool 1.\n",
    "\n",
    "The data can be split into the following subdivisions:\n",
    "1. Wowhead comments (~700 files, 31MB)\n",
    "    1. Raw .njson files containing every comment on the wowhead character pages along with some metadata, like dates. (see `download_character_comments.py`)\n",
    "        1. Note: For some characters, multiple pages are present in which we got comments from all of them.\n",
    "    2. Processed .txt files, which consists of the comments from the raw files without any metadata (see `comments_clean.py`)\n",
    "    3. Words .txt files, which based on the processed .txt files have had stopwords removed, text has been tokenized and every word has been lemmatized (see `comments_to_words.py`)\n",
    "2. Wowpedia pages (~1000 files, 19MB)\n",
    "    1. Raw .txt files containing the entire wowpedia character page (see `download_character_pages.py`)\n",
    "    2. Clean .txt files containing the clean version of the wowpedia character page (see `download_character_pages_clean.py`)\n",
    "    3. Quotes .txt files, which based on the raw .txt files consists of all the quotes from the characters quote section on its wowpedia page (see `extract_character_quotes.py`)\n",
    "    4. Words .txt files, which based on the clean .txt files have had stopwords removed, text has been tokenized and every word has been lemmatized (see `pages_to_words.py`)\n",
    "\n",
    "The resulting dataset consists of ~1700 files (totalling 50 MB).\n",
    "\n",
    "The resulting network graph has 261 nodes and 4009 edges (see `Graph Analysis.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, Theory and Analysis\n",
    "Describe the process of theory to insight\n",
    "- Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "- Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "- How did you use the tools to understand your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool 1: Downloading and Cleaning text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool 2: Network Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool 3: Wordclouds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool 4: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "Think critically about your creation\n",
    "- What went well?\n",
    "- What is still missing? \n",
    "- What could be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What went well**\n",
    "\n",
    "\n",
    "**Missing**\n",
    "\n",
    "\n",
    "**Improvements**\n",
    "\n",
    "If we were to improve the work, there are multiple things that could be done (possibilities are somewhat endless):\n",
    "- improve character sentiment by adding more character quotes from the wowpedia pages\n",
    "    - Some quote sections include linkings to collections of character quotes, from e.g. Warcraft 2, which could be downloaded and processed.\n",
    "    - Manually find the different quoting pages on wowpedia and process these, adding quotes to major characters. \n",
    "    - Finding quotes from outside wowpedia, e.g. the Warcraft movie script or the different Warcraft books (probably behind paywalls)\n",
    "        - Could lead to time series analysis of charcter sentiment, which could be compared to the sentiment time series of wowhead comments.\n",
    "- expanding the network to not only include the [major character page on wowpedia](https://wowpedia.fandom.com/wiki/Major_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contributions\n",
    "Who did what? You should write (just briefly) which group member was the main responsible for which elements of the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main responsibilities**\n",
    "* Janus Ivert Johansen, s173917\n",
    "    * Setting up the website \n",
    "    * Wordcloud masks\n",
    "    * Network graph visualization\n",
    "* Lucas Alexander SÃ¸rensen, s174461\n",
    "    * Downloading comments from wowhead\n",
    "    * Sentiment Analysis (Bert and VADER)\n",
    "    * Timeseries sentiment\n",
    "* Simon Amtoft Pedersen, s173936\n",
    "    * Downloading & cleaning character pages\n",
    "    * Extracting character quotes from wowpedia character pages\n",
    "    * Wordcloud computations (tf-idf etc.)\n",
    "    * Compute network measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
